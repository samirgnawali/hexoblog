<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Personal blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://vikasbhandary.com.np/"/>
  <updated>2017-11-19T15:09:07.530Z</updated>
  <id>https://vikasbhandary.com.np/</id>
  
  <author>
    <name>Vikas Bhandary</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CapsuleNet: Understanding Dynamic routing between capsules</title>
    <link href="https://vikasbhandary.com.np/CapsuleNet-Understanding-Dynamic-routing-between-capsules/"/>
    <id>https://vikasbhandary.com.np/CapsuleNet-Understanding-Dynamic-routing-between-capsules/</id>
    <published>2017-11-19T14:17:55.000Z</published>
    <updated>2017-11-19T15:09:07.530Z</updated>
    
    <content type="html"><![CDATA[<p>There are few weaknesses of Convonutional Neural Networks (CNN), which <strong>Geoffery Hilton</strong> mentioned in his famous talk about <strong><a href="https://www.youtube.com/watch?v=rTawFwUvnLE" target="_blank" rel="external">what is wrongs with CNNs</a></strong> few months ago. Recently a <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a> introduced a completely new type of neural network CapsuleNet, based on so-called capsules. A capsule is a group of neurons whose outputs represent different properties of the same entity. These network use “Dynamic Routing Between Capsules”. CapsuleNet has gained wide attention, because it introduces a completely new method, which is most likely to improve overall performance and accuracy of Deep learning algorithms in coming future.<br><a id="more"></a>(Note: I won’t cover mathematical equation or programming, but i have mentioned used equation for clarifications.)<br><!-- toc --></p>
<h1 id="CNN-Quick-intro"><a href="#CNN-Quick-intro" class="headerlink" title="CNN: Quick intro"></a>CNN: Quick intro</h1><p>Main component of CNN is convolutional layer, these layers learn and detect main features of an image using pixel input. These layers are stacked up to learn and detect even more complex features. Layers close to input learn simple features whereas higher layer combine simple features into more complex features. Higher-level features combine lower-level features as a weighted sum: activations of a preceding layer are multiplied by the following layer neuron’s weights and added, before being passed to activation nonlinearity. (For more detailed study visit <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">link</a>)</p>
<h1 id="Difference-between-CNN-and-CapsuleNet"><a href="#Difference-between-CNN-and-CapsuleNet" class="headerlink" title="Difference between CNN and CapsuleNet"></a>Difference between CNN and CapsuleNet</h1><ul>
<li>Instead of single neuron, layers of CapsuleNet consist of groups of Neurons called as Capsule. These neurons have same activity vector which represents the instantiation parameters of a specific type of entity such as an object or an object part.</li>
<li>CNN uses <a href="http://ufldl.stanford.edu/tutorial/supervised/Pooling/" target="_blank" rel="external">Max pooing</a> for discretization, which is sample-based discretization process. It’s objective is to reduce dimentionality of input representation and reduce chances of overfitting by providing abstract representation of images by allowing neurons in one layer to ignore all but the most active feature detector in a local pool in the layer below [<a href="https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks" target="_blank" rel="external">num</a>, <a href="">paper</a>]. </li>
<li>CapsuleNet uses “<strong>Routing by agreement</strong>“ approach, in which output is routed to all possible parents but it is scaleddown by coupling coefficients that sum to 1. For each possible parent, the capsule computes a “prediction vector” by multiplying its own output by a weight matrix.</li>
<li>In standard deep neural networks like AlexNet and ResNet pooling between layers that downsample is a MAX operation. But in CapsuleNet, each layer learns how to pool dynamically and mimic Hebbian learning (more detail in <a href="https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44" target="_blank" rel="external">link</a>).</li>
<li>CapsuleNet is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use.</li>
</ul>
<h1 id="Disadvantage-of-CNN"><a href="#Disadvantage-of-CNN" class="headerlink" title="Disadvantage of CNN"></a>Disadvantage of CNN</h1><p>CNN has two disadvantages:<br>– (1) Pooling in CNN gives away small amount of transitional invariance at each layer, therefore precise location of the most active feature is lost. Due to this reason a CNN classifier can classify <a href="https://vikasbhandary.com.np/assets/images/picasso.jpg">Picasso’s “Portrait of woman in d`hermine”</a> as a human face, which isn’t true.<br>– (2) CNNs cannot extrapolate their understanding of geometric relationships to radically new viewpoints. For example in Diagram 3, there are multiple images of tower of liberty. For a CNN, it is really hard to recognize as same image because it does not have build-in understanding of 3D space, but for a CapsuleNet it is much easier because these relationships are explicitly modeled. The <a href="https://openreview.net/pdf?id=HJWLfGWRb" target="_blank" rel="external">paper</a> which uses this approach was able to cut the error rate by 45% as compared to the previous state of the art, which is a huge improvement [<a href="https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b" target="_blank" rel="external">link</a>]. (for more see Geoffery Hilton’s famous his famous talk about <a href="https://www.youtube.com/watch?v=rTawFwUvnLE" target="_blank" rel="external">what is wrongs with CNNs</a>)</p>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://vikasbhandary.com.np/assets/images/picasso.jpg" alt="Diagram 1: Portrait of woman in d`hermine"><span class="caption">Diagram 1: Portrait of woman in d`hermine</span></div>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://cdn-images-1.medium.com/max/800/1*pTu8CbnA_MzRbTh6Ia87hA.png" alt="Diagram 2: To a CNN, both pictures are similar, since they both contain similar elements."><span class="caption">Diagram 2: To a CNN, both pictures are similar, since they both contain similar elements.</span></div>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://cdn-images-1.medium.com/max/1000/1*nUJl2W-YVHPIxykzpixmkA.jpeg" alt="Diagram 3: To a CNN, both pictures are similar, since they both contain similar elements."><span class="caption">Diagram 3: To a CNN, both pictures are similar, since they both contain similar elements.</span></div>
<h1 id="CapsuleNet-Network-architechture"><a href="#CapsuleNet-Network-architechture" class="headerlink" title="CapsuleNet: Network architechture"></a>CapsuleNet: Network architechture</h1><div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://cdn-images-1.medium.com/max/800/1*0NxktTeAhqNyRa411M3LXA.jpeg" alt="Diagram 4: CapsNet, the neural network using capsules"><span class="caption">Diagram 4: CapsNet, the neural network using capsules</span></div>
<p>The architecture is shallow with only two convolutional layers and one fully connected layer. Conv1 has 256, 9x9 convolution kernels with a stride of 1 and ReLU activation. This layer converts pixel intensities to the activities of local feature detectors that are then used as inputs to the primary capsules.<br>The second layer (PrimaryCapsules) is a convolutional capsule layer with 32 channels of convolutional 8D capsules (i.e. each primary capsule contains 8 convolutional units with a 9x9 kernel and a stride of 2). The final Layer (DigitCaps) has one 16D capsule per digit class and each of these capsules receives input from all the capsules in the layer below.</p>
<p>In the above diagram, Dynamic routing occurs between PrimaryCaps and DigitCaps. No routing is used between Conv1 and PrimaryCapsules.</p>
<h1 id="Calculations"><a href="#Calculations" class="headerlink" title="Calculations"></a>Calculations</h1><p>CapsuleNet uses a non-linear <strong>“squashing”</strong> function to ensure that short vectors get shrunk to almost zero length and long vectors get shrunk to a length slightly below 1 (given in eq 1 in <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a>). For all but the first layer of capsules, the total input to each capsule is a weighted sum over all “prediction vectors” from the capsules in the layer below which is produced by using eq 2 in <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a>. The coupling coefficients between a capsule and all the capsules in the layer above sum to 1 and are determined by a “routing softmax”. The initial coupling coefficients are then iteratively refined by measuring the agreement between the current output of each capsule, in the layer above and the prediction made by that capsule. [<a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">num</a>]</p>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://vikasbhandary.com.np/assets/images/CapsuleNet_alg.jpg" alt="Diagram 5: pseudo code for the dynamic routing"><span class="caption">Diagram 5: pseudo code for the dynamic routing</span></div>
<h1 id="Working-of-a-capsule-Simplefied"><a href="#Working-of-a-capsule-Simplefied" class="headerlink" title="Working of a capsule: Simplefied"></a>Working of a capsule: Simplefied</h1><div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://cdn-images-1.medium.com/max/1000/1*GbmQ2X9NQoGuJ1M-EOD67g.png" alt="Diagram 6: Summary of the internal workings of the capsule. Note that there is no bias because it is already included in the W matrix that can accommodate it and other, more complex transforms and relationships."><span class="caption">Diagram 6: Summary of the internal workings of the capsule. Note that there is no bias because it is already included in the W matrix that can accommodate it and other, more complex transforms and relationships.</span></div>
<p>The <a href="https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66" target="_blank" rel="external">post</a> compares working of traditional neuron and capsule. In order to explain working of capsule let us assume we have a simple model as shown in Diagram 6, which is being used to detect a face. Inputs u1, u2 and u3 are the outputs of capsules from lower layer.</p>
<p>Following are the 4 computational steps happening inside the capsule.</p>
<h2 id="Matrix-multiplication-of-input-vectors"><a href="#Matrix-multiplication-of-input-vectors" class="headerlink" title="Matrix multiplication of input vectors"></a>Matrix multiplication of input vectors</h2><p>The length of input vectors u1, u2 and u3 corresponds to probability that the lower layer capsules detect objects, and direction of vector corresponds to the internal states of those objects. These vectors are multiplied using corresponding weight matrices W. This weight matrices W, encodes important spatial and other relationships between lower level features and higher level features. In our case lower level features are eyes, nose and mouth, and higher level features is face. After multiplication we get the predicted position of higher level features. In our case vector û1 û2 and û3, represent where face should be according to detected position of eyes, nose and mouth. </p>
<h2 id="Scalar-weighting-of-input-vectors"><a href="#Scalar-weighting-of-input-vectors" class="headerlink" title="Scalar weighting of input vectors"></a>Scalar weighting of input vectors</h2><p>This step is somewhat similar to the method used in artificial neural networks, where the weights of inputs are learned using backpropagation. In CNN scalar weighting of input vectors is done using max pooling but in Capsules this is done using “<strong>Dynamic routing</strong>“ or “<strong>Routing by agreement</strong>“. </p>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://cdn-images-1.medium.com/max/800/1*I0i5nlFe9pd1LQ5VmOohYQ.png" alt="Diagram 7: Basics of dynamic routing"><span class="caption">Diagram 7: Basics of dynamic routing</span></div>
<p>In order to understand dynamic routing, lets understand the basic concept first. Initially the lower level capsules sends prior information, then as the time progresses, and coupling coefficient get updated the capsules with more relevent information forms parse tree as shown in Diagram 7. For example images with circle as low level details links with eye or car headlights etc but not probabily with fridge. (for more indepth explanation read <a href="https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66" target="_blank" rel="external">post</a> or watch <a href="https://youtu.be/rTawFwUvnLE?t=36m39s" target="_blank" rel="external">video</a>)</p>
<p>The <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a> describes:</p>
<blockquote>
<p>Initially, the output is routed to all possible parents but is scaled down by coupling coefficients that sum to 1. For each possible parent, the capsule computes a “prediction vector” by multiplying its own output by a weight matrix. If this prediction vector has a large scalar product with the output of a possible parent, there is top-down feedback which increases the coupling coefficient for that parent and decreasing it for other parents.</p>
</blockquote>
<p>Top-down feedback is used to update the coupling coefficient of outputs of parent capsules. This coeffient depends on the scalar product of prediction vector and activity vector. In our case, coupling coefficient for capsules 1, 2 and 3 depends on scalar product of û1 and v1, û2 and v2 and û3 and v3. In <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a>, equation (3) describes the method of calculate coupling coefficient between capsule i and all the capsules in the layer above, where bij is the scalar product of prediction vector of capsule i and activity vector of capsule j from layer l+1. This is refered to as “<strong>routing softmax</strong>“ in the paper. The value of bij is iteratively refined as shown in Diagram 5 and in <a href="https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/" target="_blank" rel="external">link</a>.</p>
<h2 id="Sum-of-weighted-input-vectors"><a href="#Sum-of-weighted-input-vectors" class="headerlink" title="Sum of weighted input vectors"></a>Sum of weighted input vectors</h2><p>This step is similar to the regular artificial neuron and represents combination of inputs. Sum of weighted input vectors of capsule j, Sj, can be calculated using eqation (2) of <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a>, which is simple summation of product of coupling coefficient and input vectors. </p>
<h2 id="Vector-to-vector-nonlinearity"><a href="#Vector-to-vector-nonlinearity" class="headerlink" title="Vector-to-vector nonlinearity"></a>Vector-to-vector nonlinearity</h2><p>This is another unique approach introduced in CapsuleNet, it uses non-linear activation function, refered to as <strong>squash</strong> function in Diagram 5. This function ensures that short vectors get shrunk to almost zero length and long vectors get shrunk to a length slightly below 1. </p>
<p>Resources:<br>I have used a lot of images and concepts from <a href="https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66" target="_blank" rel="external">this</a> <a href="https://hackernoon.com/capsule-networks-are-shaking-up-ai-heres-how-to-use-them-c233a0971952" target="_blank" rel="external">post</a>, <a href="https://arxiv.org/abs/1710.09829v1" target="_blank" rel="external">paper</a> and <a href="https://youtu.be/rTawFwUvnLE" target="_blank" rel="external">video</a>. For code visit <a href="https://github.com/deepblacksky/capsnet-tensorflow" target="_blank" rel="external">link1</a> and <a href="https://www.kaggle.com/kmader/capsulenet-on-mnist" target="_blank" rel="external">link2</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;There are few weaknesses of Convonutional Neural Networks (CNN), which &lt;strong&gt;Geoffery Hilton&lt;/strong&gt; mentioned in his famous talk about &lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rTawFwUvnLE&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;what is wrongs with CNNs&lt;/a&gt;&lt;/strong&gt; few months ago. Recently a &lt;a href=&quot;https://arxiv.org/abs/1710.09829v1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;paper&lt;/a&gt; introduced a completely new type of neural network CapsuleNet, based on so-called capsules. A capsule is a group of neurons whose outputs represent different properties of the same entity. These network use “Dynamic Routing Between Capsules”. CapsuleNet has gained wide attention, because it introduces a completely new method, which is most likely to improve overall performance and accuracy of Deep learning algorithms in coming future.&lt;br&gt;
    
    </summary>
    
    
      <category term="CapsuleNet" scheme="https://vikasbhandary.com.np/tags/CapsuleNet/"/>
    
  </entry>
  
  <entry>
    <title>Why AlphaGo zero performs so well</title>
    <link href="https://vikasbhandary.com.np/Why-alphago-zero-performs-so-well/"/>
    <id>https://vikasbhandary.com.np/Why-alphago-zero-performs-so-well/</id>
    <published>2017-11-06T15:21:18.000Z</published>
    <updated>2017-11-07T13:55:18.616Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://deepmind.com/" target="_blank" rel="external">Deepmind’s</a> <a href="https://deepmind.com/research/alphago/" target="_blank" rel="external">AlphaGo</a> made a huge leap in history of computer science, when it defeated world’s top professional players in march 2016, which was previously believed to be at least a decade away. But it didnt stop there, latest evolution of AlphaGo called <a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ" target="_blank" rel="external">“AlphaGO Zero”</a> has gone many steps further than its previous version as it has reached superhuman performance, winning 100–0 against the previous champion AlphaGo. </p>
<a id="more"></a>
<!-- toc -->
<h2 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h2><p>Alphago was initially trained on dataset of expert moves of professional GO players. The process of learnig from labled training data is called <a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" rel="external">Supervised learning</a>. AlphaGO used a 13 layer neural network (NN) called “policy network” to guess next move and similar NN called “value network” to predict the winning rate. After this step, it was made to play with itself, this type of learning is called <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="external">Reinforcement learning</a>. Then finally Alphago combines both policy and value network using algorithm called Monte Carlo tree search (MCTS) in order to find the next best move. </p>
<h2 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h2><p><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/" target="_blank" rel="external">AlphaGO Zero</a> was developed with goal to create AI with superhuman proficiency. AlphGo zero teaches itself GO by self-play reinforcement learning and doenst use the any human knowledge other than domain knowledge.<br>Other Key differences between AlphaGo zero and  are:</p>
<ul>
<li>AlphaGO zero uses one NN instead of two.</li>
<li>AlphaGO zero uses black and white stones as input instead of hand picked features.</li>
<li>AlphaGO zero uses simpler Tree search algorithm without using “rollouts”.</li>
</ul>
<p>In short, the algorithm was made more general and it didn’t even used human generated data.</p>
<h3 id="How-well-it-worked"><a href="#How-well-it-worked" class="headerlink" title="How well it worked?"></a>How well it worked?</h3><div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://storage.googleapis.com/deepmind-live-cms/documents/TrainingTime-Graph-171019-r01.gif" alt="Source Deepmind blog post"><span class="caption">Source Deepmind blog post</span></div>
<p>AlphaGo Zero outperformed <a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ" target="_blank" rel="external">AlphaGo Lee</a> only after 36 hours of training, which was trained for several months. After continuous training for 72 hours AlphaGo Zero defeted the version of AlphaGo which defeated Lee Sedol. AlphaGo Lee was trained on a network of machines using 48 tensor processing units (TPUs) whereas AlphaGo Zero was trained on a single machine using 4 TPUs only.</p>
<div class="figure center fig-100" style="width:;"><img class="fig-img" src="https://storage.googleapis.com/deepmind-live-cms/images/AlphaGo%2520Efficiency.width-1500.png" alt="Source Deepmind blog  post"><span class="caption">Source Deepmind blog  post</span></div>
<h3 id="Why-it-worked"><a href="#Why-it-worked" class="headerlink" title="Why it worked?"></a>Why it worked?</h3><h4 id="Less-complex-algorithm"><a href="#Less-complex-algorithm" class="headerlink" title="Less complex algorithm"></a>Less complex algorithm</h4><p>AlphaGo Zero reduces the complexity of overall algorithm by using only self play. Only NN used in AlphaGo Zero combines the roles of both policy network and value network into single architecture. It takes current board position and it’s history as input and outputs both move probabilities and winnning probability. </p>
<h4 id="More-efficient-Tree-search-algorithm"><a href="#More-efficient-Tree-search-algorithm" class="headerlink" title="More efficient Tree search algorithm"></a>More efficient Tree search algorithm</h4><p>MCTS in AlphaGo Zero is viewed as a powerful policy improvement operator, as its calculations are used as a way to evaluate and train the NN. MCTS performs a search for winning moves. The policy evaluation estimates the value function from many sampled trajectories. The results of this search is then used to drive the learning of the neural network. So after every game, a new and potentially improved network is selected for the next self-play game.[<a href="https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f" target="_blank" rel="external">1</a>] </p>
<h4 id="Less-Training-required"><a href="#Less-Training-required" class="headerlink" title="Less Training required"></a>Less Training required</h4><p>AlphaGo Zero uses less training (3.9 million games vs 30 millions games). The reason behind it is that AlphaGo Zero uses self play for training. The data produced by self-play had more advanced moves as compared to human counterpart. After every iteration, the network selects only the best moves, So it quickly gained the superhuman expert level.</p>
<h3 id="What-AlphaGo-Zero’s-success-means"><a href="#What-AlphaGo-Zero’s-success-means" class="headerlink" title="What AlphaGo Zero’s success means?"></a>What AlphaGo Zero’s success means?</h3><h4 id="Superhuman-AI-is-possible"><a href="#Superhuman-AI-is-possible" class="headerlink" title="Superhuman AI is possible"></a>Superhuman AI is possible</h4><p>AlphaGo Zero is the very first system that can be called as Superhuman Artificial intelligence system. Although AlphaGo and AlphaGo Zero are very domain specific system, they have mastered strategic and tactical <a href="https://deepmind.com/blog/innovations-alphago/" target="_blank" rel="external">innovations</a> in the field of GO, which humans will study for many years.</p>
<h4 id="Using-human-Generated-Data-isn’t-necessarily-the-best-approach"><a href="#Using-human-Generated-Data-isn’t-necessarily-the-best-approach" class="headerlink" title="Using human Generated Data isn’t necessarily the best approach"></a>Using human Generated Data isn’t necessarily the best approach</h4><p>AlphaGo Zero started learning from scatch and didnt need human knowledge other than the game rules. Although this might sound strange but by self-play method AlphaGo Zero was able to produce better and more accurate dataset. This demonstrate that a pure reinforcement learning approach is fully feasible without human knowledge or intervention.</p>
<h4 id="Shift-towards-narrow-AI-to-AGI"><a href="#Shift-towards-narrow-AI-to-AGI" class="headerlink" title="Shift towards narrow AI to AGI"></a>Shift towards narrow AI to AGI</h4><p>The most important <a href="https://www.youtube.com/watch?v=WXHFqTvfFSw" target="_blank" rel="external">goal</a> of Deepmind was to understand the “knowledge”. Now that AlphaGo Zero was successful in gaining superhuman knowledge, it can be easily speculated that more tech companies will now try to implement it in broad domains of problems using more generalized approach. Deepmind has already <a href="https://www.bloomberg.com/news/articles/2017-10-18/deepmind-s-superpowerful-ai-sets-its-sights-on-drug-discovery" target="_blank" rel="external">announced</a> that it will be using same approach for drug discovery.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://deepmind.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deepmind’s&lt;/a&gt; &lt;a href=&quot;https://deepmind.com/research/alphago/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AlphaGo&lt;/a&gt; made a huge leap in history of computer science, when it defeated world’s top professional players in march 2016, which was previously believed to be at least a decade away. But it didnt stop there, latest evolution of AlphaGo called &lt;a href=&quot;https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;“AlphaGO Zero”&lt;/a&gt; has gone many steps further than its previous version as it has reached superhuman performance, winning 100–0 against the previous champion AlphaGo. &lt;/p&gt;
    
    </summary>
    
    
      <category term="AlphaGo" scheme="https://vikasbhandary.com.np/tags/AlphaGo/"/>
    
  </entry>
  
  <entry>
    <title>Create your own Free Blog Site</title>
    <link href="https://vikasbhandary.com.np/Create-your-own-Free-Blog-Site/"/>
    <id>https://vikasbhandary.com.np/Create-your-own-Free-Blog-Site/</id>
    <published>2017-10-30T16:34:29.000Z</published>
    <updated>2017-11-05T16:25:44.318Z</updated>
    
    <content type="html"><![CDATA[<p>There are a lot of blogging platfroms like <a href="https://wordpress.com/" target="_blank" rel="external">wordpress</a> and <a href="https://www.blogger.com" target="_blank" rel="external">blogger</a>. Personally, I was never satisfied with the flexibility of these platforms. So I always wanted a free option to create and maintain blogs. </p>
<p>We can create and maintain our own blog by using site <a href="https://www.netlify.com/" target="_blank" rel="external">Netlify</a>. It is basically a frontend hosting site, with a lot of cool features. It lets us create simple websites free of cost and it provides many advanced features for paid users. To create a blog like the one you are visiting right now for free, we will be using <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a> which is a fast, simple &amp; powerful blog framework.</p>
<a id="more"></a>
<h2 id="Lets-Start-our-blog"><a href="#Lets-Start-our-blog" class="headerlink" title="Lets Start our blog"></a>Lets Start our blog</h2><h3 id="Fork-hexo-blog"><a href="#Fork-hexo-blog" class="headerlink" title="Fork hexo blog"></a>Fork hexo blog</h3><p>Visit this <a href="https://github.com/vksbhandary/hexoblog" target="_blank" rel="external">link</a> and fork my repository or follow following steps to create a new hexo blog.</p>
<h4 id="Setup-Hexo"><a href="#Setup-Hexo" class="headerlink" title="Setup Hexo"></a>Setup Hexo</h4><p>Execute following commands in bash to setup hexo and create new blog</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-cli -g</div><div class="line">$ hexo init blog</div><div class="line">$ <span class="built_in">cd</span> blog</div><div class="line">$ npm install</div></pre></td></tr></table></figure>
<p>For more information visit <a href="https://github.com/hexojs/hexo#installation" target="_blank" rel="external">link</a>, it has all the necessary steps to get you started.</p>
<h4 id="Install-theme"><a href="#Install-theme" class="headerlink" title="Install theme"></a>Install theme</h4><p>For my convenience i have installed theme <a href="https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak" target="_blank" rel="external">hexo-theme-tranquilpeak</a>. It is well documented and support various features. You can choose any theme you want from <a href="https://hexo.io/themes/" target="_blank" rel="external">site</a>. Execute following commands to install theme.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> themes</div><div class="line">$ git <span class="built_in">clone</span> https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak</div><div class="line">$ <span class="built_in">cd</span> hexo-theme-tranquilpeak</div><div class="line">$ npm install</div><div class="line">$ bower install</div><div class="line">$ <span class="built_in">cd</span> ../..</div><div class="line">$ npm install hexo-algoliasearch</div></pre></td></tr></table></figure>
<h3 id="Setup-blog-and-theme"><a href="#Setup-blog-and-theme" class="headerlink" title="Setup blog and theme"></a>Setup blog and theme</h3><p>There are number of things you must modify like site URL, Author name, Site title etc. The themes also provide many options to give users flexibility to change things on their blogs. Themes can support various services like analytics, facebook insights etc. Generally settings are to be set in _config.yml file.<br>In order to get more info on integrated services configuration in hexo-theme-tranquilpeak theme visit this <a href="https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak/blob/master/docs/user.md#integrated-services-configuration" target="_blank" rel="external">link</a>.</p>
<h3 id="Add-blog-to-repository"><a href="#Add-blog-to-repository" class="headerlink" title="Add blog to repository"></a>Add blog to repository</h3><p>For netlify we need to add our UI on repository, so pick any git client and upload your blog.</p>
<p>For information on creating repository on github fillow <a href="https://help.github.com/articles/create-a-repo/" target="_blank" rel="external">link</a>.</p>
<h3 id="Add-site-on-Netlify"><a href="#Add-site-on-Netlify" class="headerlink" title="Add site on Netlify"></a>Add site on Netlify</h3><p>Signup and create site using your github. Their wizerd shows three steps, 1 Connect to your git provider, 2 Pick your repository, 3 Build options, and deploy!</p>
<p>we need to provide Build options</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install; <span class="built_in">cd</span> themes/tranquilpeak; npm install; bower install; grunt buildProd ; <span class="built_in">cd</span> .. ; <span class="built_in">cd</span> .. ; hexo generate; hexo algolia;</div></pre></td></tr></table></figure>
<p>And finally set publish directory as “/public”</p>
<p>Note: In my repo i have renamed “hexo-theme-tranquilpeak” theme to “tranquilpeak” so please use your head while copying.</p>
<h3 id="Domain-Registration"><a href="#Domain-Registration" class="headerlink" title="Domain Registration"></a>Domain Registration</h3><p>In nepal, there is a company called <a href="http://www.mos.com.np/" target="_blank" rel="external">Mercantile</a> which provides .com.np domains free of cost to nepali citizens. So you can visit this <a href="http://register.mos.com.np/" target="_blank" rel="external">link</a> and fill out the registration form, and wait for two days atleast and you will get email confirmation about the domain registration. You can not choose domain of your own you must go for yournamesurname.com.np only. Other citizens can buy domain from any provider.</p>
<p>Registration process asks for nameserver, in order to access nameserver Goto Domain management-&gt;Custom Domain-&gt;Add a custom domain. Set your domain, and click “Use Netlify DNS”. After that, you will have three nameserver values, use these to set your nameserver in domain provider’s setting.</p>
<h3 id="Add-DNS-records"><a href="#Add-DNS-records" class="headerlink" title="Add DNS records"></a>Add DNS records</h3><p>You might want to use www version of your domain or vice-versa  for that you can add DNS record on Netlify so you can enable HTTPS on your site.</p>
<p>So you can add any one of following two records.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Type: CNAME</div><div class="line">Host: www.yourdomain.com.np</div><div class="line">Value: yourdomain.com.np</div><div class="line">TTL: 3600</div></pre></td></tr></table></figure>
<p>or </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Type: CNAME</div><div class="line">Host: yourdomain.com.np</div><div class="line">Value: www.yourdomain.com.np</div><div class="line">TTL: 3600</div></pre></td></tr></table></figure>
<h3 id="Enable-HTTPS"><a href="#Enable-HTTPS" class="headerlink" title="Enable HTTPS"></a>Enable HTTPS</h3><p>Goto Domain management-&gt;HTTPS and click on “Use lets encrypt” or “Set custom certificate”. After a minute or so HTTPS will be enabled.</p>
<h3 id="Force-HTTPS"><a href="#Force-HTTPS" class="headerlink" title="Force HTTPS"></a>Force HTTPS</h3><p>Goto Domain management-&gt;Force HTTPS and click on “Force HTTPS” button.</p>
<h2 id="Happy-blogging"><a href="#Happy-blogging" class="headerlink" title="Happy blogging"></a>Happy blogging</h2><p>Now you can use your blog. If you are a developper you can modify your blog for your own good.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;There are a lot of blogging platfroms like &lt;a href=&quot;https://wordpress.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;wordpress&lt;/a&gt; and &lt;a href=&quot;https://www.blogger.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;blogger&lt;/a&gt;. Personally, I was never satisfied with the flexibility of these platforms. So I always wanted a free option to create and maintain blogs. &lt;/p&gt;
&lt;p&gt;We can create and maintain our own blog by using site &lt;a href=&quot;https://www.netlify.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Netlify&lt;/a&gt;. It is basically a frontend hosting site, with a lot of cool features. It lets us create simple websites free of cost and it provides many advanced features for paid users. To create a blog like the one you are visiting right now for free, we will be using &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt; which is a fast, simple &amp;amp; powerful blog framework.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hexo blog" scheme="https://vikasbhandary.com.np/tags/Hexo-blog/"/>
    
  </entry>
  
</feed>
